<!doctype html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>⚙️ Fine-Tuning for `if` Statements</title>
    <link href="/~qhoang/index.css" rel="stylesheet" />
  </head>

  <body>
    <article><div><h1>⚙️ Fine-Tuning for <code>if</code> Statements</h1><p><a href="/~qhoang/projects">< Back to Projects</a></p><p><a href="/~qhoang/">< Back Home</a></p><p><img src="/~qhoang/images/fine-tuning.png" alt="Original to Masked Functions"></img></p><p>This project explores <b>code completing suitable</b> <code>if</code> <b>statements in Python functions</b>, leveraging <a href="https://huggingface.co/Salesforce/codet5-small">CodeT5 Transformer model from Hugging Face (small-sized model)</a>. CodeT5 is a pre-trained encoder-decoder Transformer model designed for code understanding and generation. It has been trained on a large corpus of code across multiple programming languages and supports a range of downstream tasks such as <i>code completion</i>, <i>summarization</i>, <i>translation</i>, and <i>generation</i>.</p><p><a href="https://github.com/theantigone/Fine-Tuning-CodeT5">Source Code</a></p></div></article>
  </body>
</html>
