<!doctype html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>✍️ Prompt Engineering LLMs</title>
    <link href="/~qhoang/index.css" rel="stylesheet" />
  </head>

  <body>
    <article><div><h1>✍️ Prompt Engineering LLMs</h1><p><a href="/~qhoang/projects">< Back to Projects</a></p><p><a href="/~qhoang/">< Back Home</a></p><p>This project is about <b>Prompt Engineering for In-Context Learning</b> that investigates the impact of different prompt designs on the performance of <b>Large Language Models</b> (<code>LLMs</code>) across a variety of software engineering tasks.</p><p>In this assignment, five prompting strategies—<i>zero-shot</i>, <i>few-shot</i>, <i>chain-of-thought</i>, <i>prompt-chaining</i>, and <i>self-consistency</i>—were applied to 22 tasks, including code summarization, bug fixing, API generation, and code translation.</p><p>Experiments compared four models—<a href="https://github.com/marketplace/models/azure-openai/gpt-4-1/">gpt-4.1</a>, <a href="https://github.com/marketplace/models/azureml-mistral/Codestral-2501">Codestral-2501</a>, <a href="https://github.com/marketplace/models/azure-openai/gpt-4-1-mini">gpt-4.1-mini</a>, and <a href="https://github.com/marketplace/models/azure-openai/gpt-4-1-nano">gpt-4.1-nano</a>—to demonstrate how the strategic use of prompt examples and structured reasoning influences the quality and clarity of generated code.</p><p>Self-consistency prompting employed <code>3</code> repetitions, with a temperature setting of <code>0.7</code> and a maximum token limit of <code>1024</code> tokens across all evaluations.</p><p><a href="https://github.com/theantigone/Prompt-Engineering-for-In-Context-Learning">Source Code</a></p></div></article>
  </body>
</html>
